{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "features_df:       key  hold_time  flight_time  delay_time\n",
      "0               152         1867        1833\n",
      "1               152         1867         285\n",
      "2               152         1867         448\n",
      "3               152         1867         296\n",
      "4               152         1867         203\n",
      "...   ..        ...          ...         ...\n",
      "3767   y        132          767         748\n",
      "3768   y        132          767         522\n",
      "3769   y        132          468         421\n",
      "3770   y        132          468         748\n",
      "3771   y        132          468         522\n",
      "\n",
      "[3772 rows x 4 columns]\n",
      "Keystroke features saved to 'processed_data/processed_features/jjulius_processed_featured_data.csv'\n",
      "Epoch 1/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jrius/development/flasktest/env/lib/python3.10/site-packages/keras/src/layers/rnn/rnn.py:204: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(**kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m189/189\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 7ms/step - accuracy: 0.0000e+00 - loss: -1297.8325 - val_accuracy: 0.0000e+00 - val_loss: -23893.0645\n",
      "Epoch 2/10\n",
      "\u001b[1m189/189\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.0000e+00 - loss: -41462.1836 - val_accuracy: 0.0000e+00 - val_loss: -108296.1484\n",
      "Epoch 3/10\n",
      "\u001b[1m189/189\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.0000e+00 - loss: -128300.4766 - val_accuracy: 0.0000e+00 - val_loss: -220777.7969\n",
      "Epoch 4/10\n",
      "\u001b[1m189/189\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.0000e+00 - loss: -252284.2344 - val_accuracy: 0.0000e+00 - val_loss: -362889.5938\n",
      "Epoch 5/10\n",
      "\u001b[1m189/189\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.0000e+00 - loss: -405908.3125 - val_accuracy: 0.0000e+00 - val_loss: -538415.6250\n",
      "Epoch 6/10\n",
      "\u001b[1m189/189\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.0000e+00 - loss: -577943.1250 - val_accuracy: 0.0000e+00 - val_loss: -743066.2500\n",
      "Epoch 7/10\n",
      "\u001b[1m189/189\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.0000e+00 - loss: -769707.1250 - val_accuracy: 0.0000e+00 - val_loss: -976836.3125\n",
      "Epoch 8/10\n",
      "\u001b[1m189/189\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - accuracy: 0.0000e+00 - loss: -1039642.9375 - val_accuracy: 0.0000e+00 - val_loss: -1237668.2500\n",
      "Epoch 9/10\n",
      "\u001b[1m189/189\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 7ms/step - accuracy: 0.0000e+00 - loss: -1276442.7500 - val_accuracy: 0.0000e+00 - val_loss: -1524940.5000\n",
      "Epoch 10/10\n",
      "\u001b[1m189/189\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.0000e+00 - loss: -1591844.5000 - val_accuracy: 0.0000e+00 - val_loss: -1839718.3750\n",
      "Scaled Mean 2.6372211825666497e-17 X_scaled Var 0.9999999999999999\n",
      "lstm_scaler_mean: 2.6372211825666497e-17 lstm_scaler_var: 0.9999999999999999\n",
      "\u001b[1m118/118\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step\n",
      "lstm prediction:  1.0 authenticated True\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten,LSTM, Dense, UpSampling1D,Reshape,MaxPooling1D,Conv1D,Dropout,LeakyReLU\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import os\n",
    "\n",
    "# tran and save model start\n",
    "def train_and_save_model(model, data, username, epochs=10, batch_size=32):\n",
    "    # Compile the model with a binary cross-entropy loss function and accuracy metric\n",
    "    model.compile(optimizer=Adam(), loss='binary_crossentropy', metrics=['accuracy'])\n",
    "    \n",
    "    # Train the model on the keystroke data\n",
    "    model.fit(data, data, epochs=epochs, batch_size=batch_size)\n",
    "    \n",
    "    # Save the trained model to a file and return the file path\n",
    "    model_path = f'models/{username}_model.h5'\n",
    "    os.makedirs(os.path.dirname(model_path), exist_ok=True)\n",
    "    model.save(model_path)\n",
    "    \n",
    "    return model_path\n",
    "\n",
    "# tran and save model start\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# generative function start\n",
    "def create_generator_model(input_dim):\n",
    "    # Initialize the generator model\n",
    "    model = Sequential()\n",
    "    \n",
    "    # Add dense layers to the generator\n",
    "    model.add(Dense(256, input_dim=input_dim, activation='relu'))\n",
    "    model.add(Dense(512, activation='relu'))\n",
    "    model.add(Dense(input_dim, activation='tanh'))  # Output layer\n",
    "    \n",
    "    return model\n",
    "\n",
    "def create_discriminator_model(input_dim):\n",
    "    # Initialize the discriminator model\n",
    "    model = Sequential()\n",
    "    \n",
    "    # Add dense layers to the discriminator\n",
    "    model.add(Dense(512, input_dim=input_dim, activation='relu'))\n",
    "    model.add(Dense(256, activation='relu'))\n",
    "    model.add(Dense(1, activation='sigmoid'))  # Output layer for binary classification\n",
    "    \n",
    "    return model\n",
    "\n",
    "# generative function end\n",
    "\n",
    "# Long Short-Term Memory (LSTM) Network start\n",
    "def create_lstm_model(input_shape):\n",
    "    # Initialize the LSTM model\n",
    "    model = Sequential()\n",
    "    \n",
    "    # Add LSTM layers\n",
    "    model.add(LSTM(50, return_sequences=True, input_shape=input_shape))\n",
    "    model.add(LSTM(50))\n",
    "    \n",
    "    # Add the output layer for binary classification\n",
    "    model.add(Dense(1, activation='sigmoid'))\n",
    "    \n",
    "    return model\n",
    "# Long Short-Term Memory (LSTM) Network end\n",
    "\n",
    "\n",
    "# Multi-Layer Perceptron (MLP) start\n",
    "\n",
    "def create_mlp_model(input_dim):\n",
    "    # Initialize the MLP model\n",
    "    model = Sequential()\n",
    "    \n",
    "    # Add dense layers to the MLP\n",
    "    model.add(Dense(128, input_dim=input_dim, activation='relu'))\n",
    "    model.add(Dense(64, activation='relu'))\n",
    "    model.add(Dense(1, activation='sigmoid'))  # Output layer for binary classification\n",
    "    \n",
    "    return model\n",
    "\n",
    "# Multi-Layer Perceptron (MLP) end\n",
    "\n",
    "\n",
    "# random forest layer start\n",
    "def train_random_forest_model(filepath):\n",
    "    # Load the keystroke data from the CSV file\n",
    "    df = pd.read_csv(filepath)\n",
    "    \n",
    "    # Separate the features (flight time, delay time) and the target (keystroke type)\n",
    "    X = df[['hold_time', 'flight_time']].fillna(0)\n",
    "    y = df['key']\n",
    "    \n",
    "    # Split the data into training and testing sets\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "    \n",
    "    # Train the Random Forest classifier\n",
    "    rf_model = RandomForestClassifier(n_estimators=100)\n",
    "    rf_model.fit(X_train, y_train)\n",
    "    \n",
    "    # Evaluate the model on the testing set\n",
    "    y_pred = rf_model.predict(X_test)\n",
    "    rf_accuracy = accuracy_score(y_test, y_pred)\n",
    "    \n",
    "    return rf_model, rf_accuracy\n",
    "# random forest layer end\n",
    "\n",
    "# Convolutional Neural Network (CNN) start\n",
    "def create_cnn_model(input_shape):\n",
    "    # Initialize the CNN model\n",
    "    model = Sequential()\n",
    "    \n",
    "    # Add convolutional and pooling layers\n",
    "    model.add(Conv2D(9, kernel_size=(9), activation='relu', input_shape=input_shape))\n",
    "    model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "    \n",
    "    # Flatten the output and add dense layers\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(128, activation='relu'))\n",
    "    model.add(Dense(1, activation='sigmoid'))  # Output layer for binary classification\n",
    "    \n",
    "    return model\n",
    "\n",
    "# Convolutional Neural Network (CNN) end\n",
    "\n",
    "\n",
    "def process_data(data = []):\n",
    "    rf = np.array(data)\n",
    "    keystrokes = []\n",
    "    press_times = []\n",
    "    release_times = []\n",
    "    \n",
    "    for k in rf:\n",
    "        keystrokes.append(k[0])\n",
    "        if k[2] == 'keydown':\n",
    "            press_times.append(k[1])\n",
    "        elif k[2] == 'keyup':\n",
    "            release_times.append(k[1])\n",
    "    \n",
    "    print('keystrokes: ',len(keystrokes))\n",
    "    print('\\n')\n",
    "    print('press times: ',len(press_times))\n",
    "    print('\\n')\n",
    "    # print('released times: ',len(release_times))\n",
    "    flight_times = np.array(release_times) - np.array(press_times)\n",
    "    \n",
    "    # print('released times: ',len(flight_times),flight_times)\n",
    "    \n",
    "    delayed_times = np.array(press_times[1:]) - np.array(release_times[:-1])\n",
    "    print('\\n')\n",
    "    # print('delayed times: ',len(delayed_times),delayed_times)\n",
    "    \n",
    "    new_data = np.concatenate([flight_times,delayed_times])\n",
    "    print('\\n')\n",
    "    # print('fight/delayed times: ',len(new_data),new_data.shape,new_data)\n",
    "    reshaped_data = new_data.reshape(-1,1)\n",
    "    print('\\n')\n",
    "    # print('reshaped fight/delayed times: ',len(reshaped_data),reshaped_data.shape,reshaped_data)\n",
    "    \n",
    "    model = create_cnn_model(input_shape=reshaped_data.shape)\n",
    "    # model_path = train_and_save_model(model,reshaped_data,'GOD IS GOOD')\n",
    "    \n",
    "    print('\\n')\n",
    "    # print(f'model path: {model_path}')\n",
    "    \n",
    "# save processed featured data start\n",
    "def save_processed_data(username,dataPath):\n",
    "    process_dir = os.path.join('featured_data','featured_csvs')\n",
    "    if not os.path.exists(process_dir):\n",
    "        os.makedirs(process_dir)\n",
    "    processed_filepath = os.path.join(process_dir, f'{username}_processed_featured_data.csv')\n",
    "    processed_filepath2 = os.path.join(process_dir, f'{username}_processed_featured_data2.csv')\n",
    "    \n",
    "    hold_times= []\n",
    "    flight_times= []\n",
    "    last_release_time = None\n",
    "    dataFm = pd.read_csv(dataPath)\n",
    "    \n",
    "    for i in range(len(dataFm)):\n",
    "        if dataFm.iloc[i]['event'] == 'keydown':\n",
    "            release_index = dataFm[(dataFm['key']== dataFm.iloc[i]['key']) & (dataFm['event'] == 'keyup')].index\n",
    "            print('release index:', release_index)\n",
    "            if not release_index.empty:\n",
    "                hold_time = dataFm.loc[release_index[0], 'time'] - dataFm.iloc[i]['time']\n",
    "                hold_times.append(hold_time)\n",
    "            else:\n",
    "                hold_times.append(None)\n",
    "            if last_release_time is not None:\n",
    "                flight_time = dataFm.iloc[i]['time'] - last_release_time\n",
    "                flight_times.append(flight_time)\n",
    "            else:\n",
    "                flight_times.append(None)\n",
    "        elif dataFm.iloc[i]['event'] == 'keyup':\n",
    "            last_release_time = dataFm.iloc[i]['time']\n",
    "        \n",
    "    data = {'hold_time': hold_times, 'flight_time': flight_times}\n",
    "    # print('DATA',data)\n",
    "    \n",
    "    dataFm['hold_time'] = pd.Series(hold_times)\n",
    "    dataFm['flight_time'] = pd.Series(flight_times)\n",
    "    \n",
    "    dataFm.to_csv(processed_filepath2, index=False)\n",
    "    \n",
    "            \n",
    "    \n",
    "# save processed featured data end\n",
    "\n",
    "# generate features start\n",
    "def generate_features(username,filepath):\n",
    "    # Generate features here\n",
    "\n",
    "    process_dir = os.path.join('processed_data','processed_features')\n",
    "    if not os.path.exists(process_dir):\n",
    "        os.makedirs(process_dir)\n",
    "    processed_filepath = os.path.join(process_dir, f'{username}_processed_featured_data.csv')\n",
    "    # Load the raw keystroke data\n",
    "    raw_data = pd.read_csv(filepath)\n",
    "\n",
    "    # Sort data by timestamp to ensure correct order\n",
    "    raw_data = raw_data.sort_values(by='time')\n",
    "\n",
    "    # Initialize lists to store computed features\n",
    "    hold_times = []\n",
    "    flight_times = []\n",
    "    delay_times = []\n",
    "\n",
    "    # Initialize variables to keep track of previous key events\n",
    "    prev_keyup_time = None\n",
    "    prev_keydown_time = None\n",
    "    prev_key = None\n",
    "\n",
    "    # Process the raw keystroke data\n",
    "    for index, row in raw_data.iterrows():\n",
    "        key = row['key']\n",
    "        event = row['event']\n",
    "        timestamp = row['time']\n",
    "        \n",
    "        if event == 'keydown':\n",
    "            # Calculate delay time (time between the previous keydown and current keydown)\n",
    "            if prev_keydown_time is not None:\n",
    "                delay_time = timestamp - prev_keydown_time\n",
    "                delay_times.append({'key': key, 'delay_time': delay_time})\n",
    "            \n",
    "            # Update the previous keydown timestamp\n",
    "            prev_keydown_time = timestamp\n",
    "        \n",
    "        elif event == 'keyup':\n",
    "            # Calculate hold time (time between keydown and keyup of the same key)\n",
    "            hold_time = timestamp - prev_keydown_time\n",
    "            hold_times.append({'key': key, 'hold_time': hold_time})\n",
    "            \n",
    "            # Calculate flight time (time between the previous keyup and the current keydown)\n",
    "            if prev_keyup_time is not None:\n",
    "                flight_time = timestamp - prev_keyup_time\n",
    "                flight_times.append({'key': key, 'flight_time': flight_time})\n",
    "            \n",
    "            # Update the previous keyup timestamp\n",
    "            prev_keyup_time = timestamp\n",
    "\n",
    "    # Convert the lists to DataFrames\n",
    "    hold_times_df = pd.DataFrame(hold_times)\n",
    "    flight_times_df = pd.DataFrame(flight_times)\n",
    "    delay_times_df = pd.DataFrame(delay_times)\n",
    "    # print(\"hold_times: \",hold_times)\n",
    "    # print(\"\\n\")\n",
    "    # print(\"flight_times: \", flight_times)\n",
    "    # print(\"\\n\")\n",
    "\n",
    "    # print(\"delay_times: \",delay_times)\n",
    "    # print(\"\\n\")\n",
    "\n",
    "    # Merge all the DataFrames on the 'key' column\n",
    "    features_df = pd.merge(hold_times_df, flight_times_df, on='key', how='outer')\n",
    "    features_df = pd.merge(features_df, delay_times_df, on='key', how='outer')\n",
    "    \n",
    "    print(\"\\n\")\n",
    "    print(\"features_df: \",features_df)\n",
    "\n",
    "    # Save the features to a CSV file\n",
    "    features_df.to_csv(processed_filepath, index=False)\n",
    "\n",
    "    print(f\"Keystroke features saved to '{processed_filepath}'\")\n",
    "    \"\"\"\n",
    "    Explanation of the Script\n",
    "\n",
    "    Loading and Sorting:\n",
    "        The script starts by loading the raw keystroke data from a CSV file and sorting it by timestamp to ensure the events are processed in chronological order.\n",
    "\n",
    "    Feature Computation:\n",
    "        Hold Time: The script calculates the time a key is held down by finding the difference between the keydown and keyup events for the same key.\n",
    "        Flight Time: The time between releasing one key and pressing the next key is computed.\n",
    "        Delay Time: The time between pressing two consecutive keys is calculated.\n",
    "\n",
    "    Data Merging:\n",
    "        The computed features are stored in separate lists, which are then converted to pandas DataFrames.\n",
    "        These DataFrames are merged on the key column to form a single DataFrame containing all the features.\n",
    "    \"\"\"\n",
    "    return processed_filepath\n",
    "\n",
    "# generate features end\n",
    "\n",
    "# create cnn model start\n",
    "def create_cnn_model(input_shape):\n",
    "    model = Sequential()\n",
    "    \n",
    "    # Encoder\n",
    "    model.add(Conv1D(filters=16, kernel_size=2, activation='relu', input_shape=input_shape, padding='same'))\n",
    "    model.add(MaxPooling1D(pool_size=2, padding='same'))\n",
    "    model.add(Conv1D(filters=8, kernel_size=2, activation='relu', padding='same'))\n",
    "    model.add(MaxPooling1D(pool_size=2, padding='same'))\n",
    "    \n",
    "    # Flatten and bottleneck layer\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(8, activation='relu'))  # Bottleneck layer\n",
    "    \n",
    "    # Decoder\n",
    "    model.add(Dense(8 * (input_shape[0] // 4), activation='relu'))\n",
    "    model.add(Reshape((input_shape[0] // 4, 8)))\n",
    "    model.add(UpSampling1D(2))\n",
    "    model.add(Conv1D(filters=8, kernel_size=2, activation='relu', padding='same'))\n",
    "    model.add(UpSampling1D(2))\n",
    "    model.add(Conv1D(filters=16, kernel_size=2, activation='relu', padding='same'))\n",
    "    model.add(Conv1D(filters=1, kernel_size=2, activation='sigmoid', padding='same'))\n",
    "    \n",
    "    model.compile(optimizer='adam', loss='mse')\n",
    "    return model\n",
    "# create cnn model end\n",
    "\n",
    "\n",
    "# train and save model start\n",
    "def load_preprocess_data_train_model(username,filepath):\n",
    "    # Load the keystroke data\n",
    "    data = pd.read_csv(filepath)\n",
    "    key_out = data.drop('key',axis=1)\n",
    "    key_out = key_out.drop('hold_time',axis=1)\n",
    "    key_out = key_out.drop('flight_time',axis=1)\n",
    "    # Use all data as features (no label column)\n",
    "    X = key_out.values\n",
    "\n",
    "    # Split data for testing (here using 80% training and 20% testing)\n",
    "    X_train, X_test = train_test_split(X, test_size=0.2, random_state=42)\n",
    "\n",
    "    # Standardize\n",
    "    scaler = StandardScaler()\n",
    "    X = scaler.fit_transform(X.reshape(-1, X.shape[-1])).reshape(X.shape)\n",
    "\n",
    "    # Train/test split\n",
    "    X_train, X_test = train_test_split(X, test_size=0.2, random_state=42)\n",
    "\n",
    "    # Reshape if necessary for the CNN\n",
    "    X_train = X_train.reshape(X_train.shape[0], X_train.shape[1], 1)\n",
    "    X_test = X_test.reshape(X_test.shape[0], X_test.shape[1], 1)\n",
    "    \n",
    "    # Create the model\n",
    "    model = create_cnn_model(X_train.shape[1:])\n",
    "\n",
    "    # Model summary\n",
    "    model.summary()\n",
    "    \n",
    "    # Train the model\n",
    "    model.fit(X_train, X_train, epochs=50, batch_size=32, validation_data=(X_test, X_test))\n",
    "\n",
    "    model_dir = os.path.join('trained_models','model_files')\n",
    "    if not os.path.exists(model_dir):\n",
    "        os.makedirs(model_dir)\n",
    "    model_filepath = os.path.join(model_dir, f'{username}_keystroke_auth_model.keras')\n",
    "    # Save the trained model\n",
    "    model.save(model_filepath)\n",
    "    \n",
    "    # Detect anomalies\n",
    "    # Load the trained autoencoder model\n",
    "    autoencoder = tf.keras.models.load_model(model_filepath)\n",
    "\n",
    "    # Compute reconstruction error on test data\n",
    "    reconstructions = autoencoder.predict(X_test)\n",
    "    # reconstructions = autoencoder.predict(X)\n",
    "    mse = np.mean(np.power(X_test - reconstructions, 2), axis=1)\n",
    "    # mse = np.mean(np.power(X - reconstructions, 2), axis=1)\n",
    "\n",
    "    # Define a threshold for anomaly detection\n",
    "    threshold = np.percentile(mse, 95)  # For example, use the 95th percentile as the threshold\n",
    "\n",
    "    # Predict anomalies\n",
    "    anomalies = mse > threshold\n",
    "    \n",
    "    print(\"\\n\\n anomalies\",anomalies)\n",
    "    print(\"\\n\\n reconstructions:-\",reconstructions)\n",
    "    \n",
    "    \n",
    "    # if mse > threshold:\n",
    "    #     print(f\"Anomaly detection threshold: {threshold} (imposter)\")\n",
    "    #     print(\"\\n\")\n",
    "    # else:\n",
    "    #     print(f\"Anomaly detection threshold: {threshold} (genuine user)\")\n",
    "    #     print(\"\\n\")\n",
    "\n",
    "    # Output results\n",
    "    # for i, is_anomaly in enumerate(anomalies):\n",
    "    #     if is_anomaly:\n",
    "    #         print(f\"Test sample {i} is an anomaly (likely an impostor).\")\n",
    "    #         print(\"\\n\")\n",
    "    #     else:\n",
    "    #         print(f\"Test sample {i} is normal (likely a genuine user).\")\n",
    "    #         print(\"\\n\")\n",
    "\n",
    "# train and save model end\n",
    "\n",
    "\n",
    "# create Multi-Layer Perceptron (MLP) model start\n",
    "def create_mlp_model(input_shape):\n",
    "    model = Sequential()\n",
    "    model.add(Dense(64, input_shape=(input_shape,), activation='relu'))\n",
    "    model.add(Dropout(0.5))\n",
    "    model.add(Dense(128, activation='relu'))\n",
    "    model.add(Dropout(0.5))\n",
    "    model.add(Dense(1, activation='sigmoid'))\n",
    "    \n",
    "    model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "    return model\n",
    "# create Multi-Layer Perceptron (MLP) model end\n",
    "\n",
    "\n",
    "# train and save mlp model start\n",
    "def preprocess_mlp_data(data):\n",
    "    X = data.drop('key', axis=1).values  # Features\n",
    "    y = data.drop('key',axis=1)['delay_time'].values  # Labels\n",
    "    \n",
    "    # Split the data into train and test sets\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "    \n",
    "    # Standardize the data\n",
    "    scaler = StandardScaler()\n",
    "    X_train = scaler.fit_transform(X_train)\n",
    "    X_test = scaler.transform(X_test)\n",
    "    \n",
    "    return X_train, X_test, y_train, y_test\n",
    "    \n",
    "def load_preprocess_data_train_mlp_model(username,filepath):\n",
    "    # Load the keystroke data\n",
    "    data = pd.read_csv(filepath)\n",
    "    X_train, X_test, y_train, y_test = preprocess_mlp_data(data)\n",
    "    \n",
    "    print(\"X_train.shape: \",X_train.shape,\"X_test.shape: \",X_test.shape)\n",
    "    \n",
    "    model = create_mlp_model(X_train.shape[1])\n",
    "    model.fit(X_train, y_train, epochs=10, batch_size=32, validation_data=(X_test, y_test))\n",
    "    \n",
    "    # Save the trained model\n",
    "    model_dir = os.path.join('trained_models','model_files')\n",
    "    if not os.path.exists(model_dir):\n",
    "        os.makedirs(model_dir)\n",
    "    model_filepath = os.path.join(model_dir, f'{username}_keystroke_auth_mlp_model.keras')\n",
    "    # Save the trained model\n",
    "    model.save(model_filepath)\n",
    "    model.summary\n",
    "    \n",
    "    # Detect anomalies\n",
    "    # Load the trained autoencoder model\n",
    "    autoencoder = tf.keras.models.load_model(model_filepath)\n",
    "\n",
    "    # Compute reconstruction error on test data\n",
    "    reconstructions = autoencoder.predict(X_test)\n",
    "    # reconstructions = autoencoder.predict(X)\n",
    "    mse = np.mean(np.power(X_test - reconstructions, 2), axis=1)\n",
    "    # mse = np.mean(np.power(X - reconstructions, 2), axis=1)\n",
    "\n",
    "    # Define a threshold for anomaly detection\n",
    "    threshold = np.percentile(mse, 95)  # For example, use the 95th percentile as the threshold\n",
    "\n",
    "    # Predict anomalies\n",
    "    anomalies = mse > threshold\n",
    "    \n",
    "    # Output results\n",
    "    print(\"\\n\\nreconstructions: \",reconstructions)\n",
    "    \n",
    "    \n",
    "    # if mse > threshold:\n",
    "    #     print(f\"Anomaly detection threshold: {threshold} (imposter)\")\n",
    "    #     print(\"\\n\")\n",
    "    # else:\n",
    "    #     print(f\"Anomaly detection threshold: {threshold} (genuine user)\")\n",
    "    #     print(\"\\n\")\n",
    "\n",
    "    # Output results\n",
    "    # for i, is_anomaly in enumerate(anomalies):\n",
    "    #     if is_anomaly:\n",
    "    #         print(f\"Test sample {i} is an anomaly (likely an impostor).\")\n",
    "    #         print(\"\\n\")\n",
    "    #     else:\n",
    "    #         print(f\"Test sample {i} is normal (likely a genuine user).\")\n",
    "    #         print(\"\\n\")\n",
    "            \n",
    "    print(\"\\n\\nanomalies: \", anomalies)\n",
    "    \n",
    "    \n",
    "#train model mlp end\n",
    "\n",
    "# train and create gan model start\n",
    "def preprocess_gan_data(data):\n",
    "    X = data.drop('key', axis=1).values  # Features\n",
    "    y = data.drop('key',axis=1)['delay_time'].values  # Labels\n",
    "    # Split the data into train and test sets\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "    \n",
    "    # Standardize the data\n",
    "    scaler = StandardScaler()\n",
    "    X_train = scaler.fit_transform(X_train)\n",
    "    X_test = scaler.transform(X_test)\n",
    "    \n",
    "    return X_train, X_test, y_train, y_test\n",
    "\n",
    "# Build the generator model\n",
    "def build_generator(latent_dim):\n",
    "    model = Sequential()\n",
    "    model.add(Dense(16, input_dim=latent_dim))\n",
    "    model.add(LeakyReLU(alpha=0.2))\n",
    "    model.add(Dense(32))\n",
    "    model.add(LeakyReLU(alpha=0.2))\n",
    "    model.add(Dense(3, activation='linear'))  # Output: key_hold_time, key_flight_time, key_release_time\n",
    "    return model\n",
    "\n",
    "# Build the discriminator model\n",
    "def build_discriminator(input_shape):\n",
    "    model = Sequential()\n",
    "    model.add(Dense(32, input_shape=input_shape))\n",
    "    model.add(LeakyReLU(alpha=0.2))\n",
    "    model.add(Dense(16))\n",
    "    model.add(LeakyReLU(alpha=0.2))\n",
    "    model.add(Dense(1, activation='sigmoid'))  # Output: probability of being genuine\n",
    "    model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "# Build the GAN model\n",
    "def build_gan(generator, discriminator):\n",
    "    discriminator.trainable = False\n",
    "    model = Sequential()\n",
    "    model.add(generator)\n",
    "    model.add(discriminator)\n",
    "    model.compile(optimizer='adam', loss='binary_crossentropy')\n",
    "    return model\n",
    "# Route for generating synthetic keystroke data\n",
    "def generate_synthetic_data(featured_path,model_gan_generator_filepath):\n",
    "    latent_dim = 10\n",
    "    generator = tf.keras.models.load_model(model_gan_generator_filepath)\n",
    "    data = pd.read_csv(featured_path)\n",
    "    X_train, X_test, y_train, y_test = preprocess_gan_data(data)\n",
    "    print(\"X_train\",X_train.tolist())\n",
    "    \n",
    "    latent_points = np.random.normal(0, 1, (len(X_train), latent_dim))\n",
    "    \n",
    "    generated_data = generator.predict(latent_points)\n",
    "    \n",
    "    # Build and train authentication model\n",
    "    auth_model = Sequential([\n",
    "        Dense(64, activation='relu', input_shape=(generated_data.shape[1],)),\n",
    "        Dense(32, activation='relu'),\n",
    "        Dense(1, activation='sigmoid')\n",
    "    ])\n",
    "    \n",
    "    auth_model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "    auth_model.fit(generated_data, y_train, epochs=10, batch_size=32, validation_split=0.1)\n",
    "    \n",
    "    predictions = auth_model.predict(generated_data)\n",
    "    \n",
    "    print(\"prediction\", np.mean(predictions))\n",
    "    \n",
    "    print(\"generated_data\",generated_data.tolist())\n",
    "\n",
    "def load_preprocess_data_train_gan_model(username,filepath):\n",
    "    data = pd.read_csv(filepath)\n",
    "    X_train, X_test, y_train, y_test = preprocess_gan_data(data)\n",
    "    \n",
    "    latent_dim = 10  # Dimensionality of the latent space\n",
    "    generator = build_generator(latent_dim)\n",
    "    discriminator = build_discriminator((X_train.shape[1],))\n",
    "    gan = build_gan(generator, discriminator)\n",
    "    \n",
    "    # Training parameters\n",
    "    epochs = 200\n",
    "    batch_size = 32\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "         # Train discriminator\n",
    "        idx = np.random.randint(0, X_train.shape[0], batch_size)\n",
    "        real_features = X_train[idx]\n",
    "        fake_features = generator.predict(np.random.normal(0, 1, (batch_size, latent_dim)))\n",
    "        d_loss_real = discriminator.train_on_batch(real_features, np.ones((batch_size, 1)))\n",
    "        d_loss_fake = discriminator.train_on_batch(fake_features, np.zeros((batch_size, 1)))\n",
    "        \n",
    "        # Train generator\n",
    "        g_loss = gan.train_on_batch(np.random.normal(0, 1, (batch_size, latent_dim)), np.ones((batch_size, 1)))\n",
    "        print(f\"{epoch}/{epochs} [D loss: {0.5 * (d_loss_real[0] + d_loss_fake[0])} | D accuracy: {100 * 0.5 * (d_loss_real[1] + d_loss_fake[1])}%] [G loss: {g_loss}]\")\n",
    "        # # Train discriminator\n",
    "        # real_samples = X_train[np.random.randint(0, X_train.shape[0], batch_size)]\n",
    "        # real_labels = np.ones((batch_size, 1))\n",
    "        \n",
    "        # latent_points = np.random.normal(0, 1, (batch_size, latent_dim))\n",
    "        # fake_samples = generator.predict(latent_points)\n",
    "        # fake_labels = np.zeros((batch_size, 1))\n",
    "        \n",
    "        # discriminator_loss_real = discriminator.train_on_batch(real_samples, real_labels)\n",
    "        # discriminator_loss_fake = discriminator.train_on_batch(fake_samples, fake_labels)\n",
    "        \n",
    "        # # Train generator (via GAN model)\n",
    "        # gan_loss = gan.train_on_batch(latent_points, real_labels)\n",
    "        \n",
    "        # if (epoch + 1) % 200 == 0:\n",
    "            \n",
    "        #     print(f\"Epoch {epoch + 1}/{epochs},\")\n",
    "        #     print(f\"Discriminator Loss: {discriminator_loss_real[0]:.4f},\")\n",
    "        #     print(f\"GAN Loss:\",gan_loss)\n",
    "    \n",
    "    # Save the models\n",
    "    model_dir = os.path.join(\"trained_models\",\"gen_model_files\")\n",
    "    if not os.path.exists(model_dir):\n",
    "        os.makedirs(model_dir)\n",
    "    model_gan_generator_filepath = os.path.join(model_dir,f'{username}_keystroke_gan_generator.keras')\n",
    "    model_gan_discriminator_filepath = os.path.join(model_dir,f'{username}_keystroke_gan_discriminator.keras')\n",
    "    generator.save(model_gan_generator_filepath)\n",
    "    discriminator.save(model_gan_discriminator_filepath)\n",
    "    generator.summary\n",
    "    discriminator.summary\n",
    "    generate_synthetic_data(filepath,model_gan_generator_filepath)\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "# train and create gan model end\n",
    "\n",
    "# train and create Long Short-Term Memory (LSTM) Network model start\n",
    "def build_lstm_model(input_shape):\n",
    "    model = Sequential()\n",
    "    model.add(LSTM(64, input_shape=input_shape, return_sequences=True))\n",
    "    model.add(LSTM(32))\n",
    "    model.add(Dense(32, activation='relu'))\n",
    "    model.add(Dense(1, activation='sigmoid'))  # Output layer for binary classification\n",
    "    model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "def process_lstm_data(data):\n",
    "    X = data.drop('key', axis=1).values  # Features\n",
    "    y = data.drop('key',axis=1)['delay_time'].values  # Labels\n",
    "    \n",
    "    # standarize feautures\n",
    "    scaler = StandardScaler()\n",
    "    X_scaled = scaler.fit_transform(X)\n",
    "    \n",
    "    # Split data into training and testing sets\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X_scaled, y, test_size=0.2, random_state=42)\n",
    "\n",
    "    # Reshape input data for LSTM [samples, time steps, features]\n",
    "    X_train = np.reshape(X_train, (X_train.shape[0], 1, X_train.shape[1]))\n",
    "    X_test = np.reshape(X_test, (X_test.shape[0], 1, X_test.shape[1]))\n",
    "    \n",
    "    return X_train, X_test, y_train, y_test,X_scaled\n",
    "\n",
    "def lstm_prediction_users_auth(username,filepath):\n",
    "    # Load the saved model\n",
    "    data = pd.read_csv(filepath)\n",
    "    key_features =  data.drop('key',axis=1).values  # Labels\n",
    "    model_dir = os.path.join(\"trained_models\",\"lstm_model_files\")\n",
    "    lstm_model_filepath = os.path.join(model_dir,f'{username}_keystroke_lstm_model.keras')\n",
    "    lstm_model = tf.keras.models.load_model(lstm_model_filepath)\n",
    "    \n",
    "    # Load the scaler\n",
    "    lstm_scaler_path = os.path.join(model_dir,f'{username}_scaler_numpy.npy')\n",
    "    lstm_scaler_mean = np.load(lstm_scaler_path, allow_pickle=True)\n",
    "    lstm_scaler_var = np.load(lstm_scaler_path.replace(\".npy\",\"_var.npy\"), allow_pickle=True)\n",
    "    if not isinstance(lstm_scaler_mean, np.ndarray) or not isinstance(lstm_scaler_var, np.ndarray):\n",
    "        print(\"Failed to load scalar parameters\")\n",
    "        return\n",
    "    print(\"lstm_scaler_mean:\",lstm_scaler_mean,\"lstm_scaler_var:\",lstm_scaler_var)\n",
    "    scaler = StandardScaler()\n",
    "    scaler.mean_ = lstm_scaler_mean\n",
    "    scaler.var_ = lstm_scaler_var\n",
    "    scaler.scale_ = np.sqrt(lstm_scaler_var)\n",
    "    \n",
    "    scaler.n_samples_seen_ = 0\n",
    "    key_features = scaler.fit_transform(key_features)\n",
    "    \n",
    "    key_features = np.reshape(key_features, (key_features.shape[0], 1, key_features.shape[1]))  # Reshape for LSTM\n",
    "    \n",
    "    predictions = lstm_model.predict(key_features)\n",
    "    authenticated = (predictions[0][0] > 0.5)\n",
    "    \n",
    "    # Process new data\n",
    "    #...\n",
    "    \n",
    "    # Apply scaler\n",
    "    #...\n",
    "    \n",
    "    # Predict the delay time\n",
    "    #...\n",
    "    \n",
    "    # Return the prediction\n",
    "    \n",
    "    print(\"lstm prediction: \",np.mean(predictions),\"authenticated\",authenticated)\n",
    "    \n",
    "    \n",
    "def load_preprocess_data_train_lstm_model(username,filepath):\n",
    "    data = pd.read_csv(filepath)\n",
    "    \n",
    "    X_train, X_test, y_train,y_test,X_scaled = process_lstm_data(data)\n",
    "    lstm_model = build_lstm_model((1, X_train.shape[2]))\n",
    "    lstm_model.fit(X_train, y_train, epochs=10, batch_size=16, validation_data=(X_test, y_test))\n",
    "    # Save the models\n",
    "    model_dir = os.path.join(\"trained_models\",\"lstm_model_files\")\n",
    "    if not os.path.exists(model_dir):\n",
    "        os.makedirs(model_dir)\n",
    "    lstm_model_filepath = os.path.join(model_dir,f'{username}_keystroke_lstm_model.keras')\n",
    "    lstm_model.save(lstm_model_filepath)\n",
    "    \n",
    "    # Save the model and scaler for later use\n",
    "    print(\"Scaled Mean\",np.mean(X_scaled),\"X_scaled Var\",np.var(X_scaled))\n",
    "    lstm_scaler_path = os.path.join(model_dir,f'{username}_scaler_numpy.npy')\n",
    "    np.save(lstm_scaler_path,np.mean(X_scaled))\n",
    "    np.save(lstm_scaler_path.replace(\".npy\",\"_var.npy\"),np.var(X_scaled))\n",
    "    \n",
    "    lstm_model.summary\n",
    "    \n",
    "    lstm_prediction_users_auth(username,filepath)\n",
    "# train and create Long Short-Term Memory (LSTM) Network model end\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    \n",
    "    \n",
    "\n",
    "    \n",
    "\n",
    "def process_items():\n",
    "    # data = pd.read_csv(\"../data/raw/jjulius.csv\")\n",
    "    # print(\"raw data: \",data)\n",
    "    featured_path =  generate_features(\"jjulius\",\"../data/raw/jjulius.csv\")\n",
    "    # load_preprocess_data_train_model(\"jjulius\",featured_path)\n",
    "    # load_preprocess_data_train_mlp_model(\"jjulius\",featured_path)\n",
    "    # load_preprocess_data_train_gan_model(\"jjulius\",featured_path)\n",
    "    load_preprocess_data_train_lstm_model(\"jjulius\",featured_path)\n",
    "    \n",
    "    # save_processed_data('jjulius',\"../data/raw/jjulius.csv\")\n",
    "    # process_data(data)\n",
    "    # print(data)\n",
    "    \n",
    "if __name__ == \"__main__\":\n",
    "    process_items()\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
